---
title: SVD分解及其应用
date: 2021-09-16 18:00:00
permalink: /pages/RlgDI5
categories: 
  - 深度学习
tags: 
  - SVD
author: 
  name: yangxin
  link: https://github.com/yangxin6/DeepLearning
---

SVD（ Singular Value Decomposition）也是对矩阵进行分解，SVD不要求要分解的矩阵为方阵。假设我们的矩阵A是一个$m×n$的矩阵，那么我们定义矩阵A的SVD为：

$A=U\Sigma V^T$

其中U是一个$m×m$的矩阵， $\Sigma$ 是一个$m×n$的矩阵，除了主对角线上的元素以外全为0，<mark>主对角线</mark>上的每个元素都称为<mark>奇异值</mark>，$V$是一个$n×n$的矩阵。$U$和$V$都是<mark>酉矩阵</mark>，即满足 $U^TU=I,V^TV=I,U^TU=I,V^TV=I$。下图可以很形象的看出上面SVD的定义：

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/1042406-20170105115457425-1545975626.76l3jj3sr6o0.png" style="zoom:75%;" />

## 求SVD分解后的$U,\Sigma ,V$这三个矩阵

如果我们将$A$的转置和$A$做矩阵乘法，那么会得到$n×n$的一个方阵$A^TA$。既然$ATA$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：

$(A^TA)v_i=\lambda _iv_i$

这样我们就可以得到矩阵$A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将$A^TA$的所有特征向量张成一个$n×n$的矩阵$V$，就是我们SVD公式里面的$V$矩阵了。一般我们将$V$中的每个特征向量叫做<mark>$A$ 的右奇异向量</mark>。

如果我们将$A$和$A$的转置做矩阵乘法，那么会得到$m×m$的一个方阵$AA^T$。既然$AA^T$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：

$(AA^T)u_i=\lambda _iu_i$

这样我们就可以得到矩阵$AA^T$的m个特征值和对应的m个特征向量$u$了。将$AA^T$的所有特征向量张成一个$m×m$的矩阵$U$，就是我们SVD公式里面的$U$矩阵了。一般我们将$U$中的每个特征向量叫做<mark>$A$的左奇异向量</mark>。

$U$和$V$我们都求出来了，现在就剩下奇异值矩阵$\Sigma$没有求出了。由于$\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\Sigma$就可以了。

我们注意到:

$A=U\Sigma V^T\Rightarrow AV=U\Sigma V^TV\Rightarrow AV=U\Sigma \Rightarrow Av_i=\Sigma _iu_i\Rightarrow \Sigma _i=Av_i/u_i$

 这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵$\Sigma$。

上面还有一个问题没有讲，就是我们说$A^TA$的特征向量组成的就是我们SVD中的$V$矩阵，而$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵，这有什么根据吗？这个其实很容易证明，我们以$V$矩阵的证明为例。

$A=U\Sigma V^T\Rightarrow A^T=V\Sigma ^TU^T\Rightarrow A^TA=V\Sigma ^TU^TU\Sigma V^T=V\Sigma ^2V^T$

上式证明使用了: $U^TU=I,\Sigma ^T\Sigma =\Sigma ^2$。可以看出$A^TA$的特征向量组成的的确就是我们SVD中的$V$矩阵。类似的方法可以得到$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵。

进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：

$\Sigma _i=\sqrt{\lambda _i}$

这样也就是说，我们可以不用$\Sigma _i=Av_i/u_i$来计算奇异值，也可以通过<mark>求出$ATA$的特征值取平方根来求奇异值</mark>。

## SVD的一些性质

对于奇异值，它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用<mark>最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵</mark>。也就是说：

$A_{m×n}=U_{m×m}\Sigma _{m×n}V_{n×n}^T≈U_{m×k}\Sigma _{k×k}V_{k×n}^T$

其中$k$要比$n$小很多，也就是一个大的矩阵$A$可以用三个小的矩阵$U_{m×k}\Sigma _{k×k}V_{k×n}^T$来表示。如下图所示，现在我们的矩阵$A$只需要灰色的部分的三个小矩阵就可以近似描述了。

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/1042406-20170105140822191-1774139119.52f9pl3lqac0.png" style="zoom:75%;" />



```md
::: 致谢
特征值和特征向量以及奇异值分解，借鉴于 [刘建平Pinard](https://www.cnblogs.com/pinard/p/6251584.html) 作者的博客
:::
```

## 应用

- 图片压缩

- 神经网络中运算加速（矩阵变小）
