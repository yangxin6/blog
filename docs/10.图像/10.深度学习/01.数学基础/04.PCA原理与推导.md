---
title: PCA原理与推导
date: 2021-09-17 10:00:00
permalink: /pages/1Ro5k6
categories: 
  - 深度学习
tags: 
  - PCA
author: 
  name: yangxin
  link: https://github.com/yangxin6/DeepLearning
---



PCA仍然是一种数据压缩算法。

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/pca0.1s7r54ojv4g0.png" alt="pca0" style="zoom:70%;" />

A点需要 x,y 两个坐标来表示，假设 A 在向量 u 上的投影点为 $A^{'}$，则$A^{'}$仅需要一个参数就能表示，就是 $OA^{'}$ 的长度（即 $A^{'}$在 u 上的坐标），我们就想着用 $A^{'}$ 来替换 A，这样 N 个点（原来需要 2 * N 个参数表示），现在只需要（N+2）个参数（u也需要2个参数）



但此时就带来了误差，如 $AA^{'}$ 和$BB^{'}$，所以我们要能够找到这样一个方向 u，使得所有原始点与投影点之间的误差最小



最小重构误差



## 样本点中心化

在做pca之前做一次中心化

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/pca1.3dtssgsxxrm0.png" alt="pca1" style="zoom:67%;" />



## 推导

$\vec{e} = \vec{x} - p_{rj} \vec{x}$

​	$= \vec{x} - <\vec{x},\vec{u}>\vec{u}$

​	$=x-(x^Tu)u；x,u\in R^n$   且 $||u||=1, u^Tu=1$



$J=||\vec{e}||^2 = e^Te = [x-(x^Tu)u]T[x-(x^Tu)u]$

​	$=[x^T-(x^Tu)u^T][x-(x^Tu)u]$

​	$= x^Tx - (x^Tu)(x^Tu) - (x^Tu)(u^Tx) + (x^Tu)^2u^Tu$

​	$=||x||^2 - (x^Tu)^2 - (x^Tu)^2 + (x^Tu)^2$

​	$=||x||^2 - (x^Tu)^2$



$min\ J \Longleftrightarrow max(x^Tu)^2$

$\Longleftrightarrow max(x^Tu)(x^Tu) \Longleftrightarrow max(u^Tx)(x^Tu)$

$\Longleftrightarrow max\ u^T(xx^T)u$

共有 N 个样本

$max \sum \limits^N_{i=1}u^T(x_ix_i^T)u=u^T(\sum \limits^N_{i=1}x_ix_i^T)u，且||u||=1$

$\Longleftrightarrow max \ u^T(X)u,s.t. ||u||=1$  



**用拉格朗日求解**：

$L(u,\lambda) = u^TXu + \lambda(1-u^Tu)$

$\frac{\partial J}{\partial u} = 0 \Rightarrow Xu-\lambda u=0$

$Xu=\lambda u$ 解出来 特征值和特征向量 $\lambda_i$，对应的就是 $u_i$

$\frac{\partial J}{\partial \lambda} = 0 \Rightarrow u^Tu=1$







