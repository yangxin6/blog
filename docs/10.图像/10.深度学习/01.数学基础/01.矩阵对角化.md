---
title: 矩阵对角化
date: 2021-09-16 17:00:00
permalink: /pages/6CPDif
categories: 
  - 深度学习
author: 
  name: yangxin
  link: https://github.com/yangxin6/DeepLearning
---

- 矩阵B（方阵）的对角化 $P^{-1}AP = B$ 其中 A 为对角矩阵，P为单位正交矩阵（$P^TP = PP^T=I,P^T=P^{-1}$）。

- $P^{-1}AP = B$  $\Rightarrow$  $B=P^TAP$ 

- 一般的矩阵不一定能对角化，但是对称矩阵一定可以对角化（特别是对称正定矩阵，得到的 $\lambda_i$ 都是正数 ）。

  **对称矩阵**: $A^T=A$

  **对称正定矩阵**： 任意列向量 x，$x^TAx \geq 0$ 

  **对称半正定矩阵**： 任意列向量 x，$x^TAx > 0$ 



## 应用

矩阵压缩表示



$B=P^TAP$ 



设 $P^T = (u_1,u_2,...,u_m), u_i \in R^n$，$A = \left( \begin{array}{1} \lambda_1 & \ & \ \\ \ & \lambda_2 & \ \\ \ & \ & \ddots \\ & & & \lambda_n \end{array} \right)$ 

则 $B = (u_1,u_2,...,u_n) \left( \begin{array}{1} \lambda_1 & \ & \ \\ \ & \lambda_2 & \ \\ \ & \ & \ddots \\ & & & \lambda_n \end{array} \right) \left( \begin{array}{1} u_1 \\ u_2 \\ \vdots \\ u_n \end{array} \right)$

$= \lambda_1 u_1 u_2^T + \lambda_2u_2u_2^T + ... + \lambda_nu_nu_n^T$





结果是 将矩阵 $B_{n \times n}$ 共 <mark> $n^2$ </mark> 个参数转为 <mark>n+1</mark> 个参数表示。



## 特征值和特征向量

$A x=\lambda x$

其中$A$是一个 $n\times n$  的实对称矩阵，$x$ 是一个 $n$ 维向量，则我们说是 $\lambda$ 矩阵$A$的一个特征值，而 $x$ 是矩阵 $A$ 的特征值 $\lambda$ 所对应的<mark>特征向量</mark>。

矩阵 $A$ 的 $n$ 个特征值$\lambda _1≤\lambda _2≤...≤\lambda _n$, 以及这 $n$ 个特征值所对应的特征向量$\{\omega_1,\omega_2,...\omega_n\}$，如果这$n$个特征向量线性无关，那么矩阵$A$就可以用下式的特征分解表示:

$A=W \Sigma W^{−1}$

其中$W$是这$n$个特征向量所张成的$n×n$维矩阵，而 $\Sigma$ 为这$n$个特征值为主对角线的$n×n$维矩阵。

一般我们会把$W$的这$n$个特征向量标准化，即满足$||\omega_i||_2=1, ||w_i||_2=1$, 或者说$\omega^T_i\omega_i=1$，此时$W$的$n$个特征向量为标准<mark>正交基</mark>，满足$W^TW=I$，即$W^T=W^{−1}$, 也就是说$W$为<mark>酉矩阵</mark>。这样我们的特征分解表达式可以写成:

$A=W\Sigma W^T$

