---
title: VoxelNet & PointPillars
date: 2021-10-03 16:00:00
permalink: /pages/6s7YXu
categories: 
  - 点云
tags: 
  - VoxelNet,PointPillars,FocalLoss
author: 
  name: yangxin
  link: https://github.com/yangxin6/3DPointCloud
---

- 多视角投影
- 使用 PointNet 构建类似图像的特征图
  - PointNet 将点云检测转换为图像检测 + 普通 2D CNN
  - Two-stage / One-stage
- Point-wise 操作
  - 用 PointNet 等逐点操作替换 2D CNN
  - Two-stage / One-stage
- 点云和图像融合



## MV3D

![mv3d](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/mv3d.6uswyhfuy6o0.jpg)

- 多视图投影 + Faster RCNN
  - BV 的 3D 提案，投影到其他视图
  - 每个视图中的 ROI 池化，结合 Cls./Reg
- 开创性的工作，但现在**不经常使用**

## VoxelNet

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/voxelnet.2os9h9yremu0.png" alt="voxelnet" style="zoom:50%;" />

1. 建立**voxel grid**（体素网格）                        ($D \times H \times W$)

2. 获取每个单元格的特征

   - PointNet                        ($C \times D \times H \times W$)

3. 3D vocel $\rightarrow$ 2D feature map

   - 3D 卷积                           ($C^{'} \times D^{'} \times H^{'} \times W^{'}$) （$D^{'}$ 可以被消除）

4. 区域提案网络  Region Proposal Network (RPN)（RPN）

   - One network for one category （每一个网络只处理一个类别）

   - RPN 只预测锚框是一个对象



### Voxel Feature Encoding (VFE)

![eg](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/eg.5u0vbk8zze80.png)



<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/voxel.142wo8sfvo2k.jpg" alt="voxel" style="zoom:47%;" />

- 10×400×352 cells
  - 90% cells are empty

1. 构建两个容器

      a)  K 最大非空体 voxels

      b) T 每个voxel的最大点数

      c) $K \times T \times 7$ 个点      ---- 容器c：存放**点的信息**

   ​		- ($x_i,y_i,z_i,r_i,x_i-v_{ix},y_i-v_{iy},z_i-v_{iz}$)（$r_i$反射率，$v_{i}$ voxel的中心点）

      d) $K \times 3$ **voxel coordinates**     ---- 容器d：存放**voxel的坐标**

2. 遍历每个点

   - 得到 voxel coordinates  $(h_x,h_y,h_z)$
     - $Hash(h_x,h_y,h_z) \rightarrow$   index $k$  in container (c)(d)
   - 如果  (c)/(d)中存在第 k 个元素，则附加到 (c)
   - 如果不存在，则在 (c) (d)中创建第 k 个元素 

3. VFE / PointNet  on (c)  $\rightarrow K \times 128$

4. Put (c) back to the voxel grid $\rightarrow$ $128\times10\times400\times352$  （只有 K 个是有点的 空的填充0）



### Convolutional Middle Layers / 3DCNN

- Input: 128×10×400×352
- Conv3D
  - Output channel # 64, kernel (3, 3, 3), stride (2, 1, 1), padding (1, 1, 1)
  - Output channel # 64, kernel (3, 3, 3), stride (1, 1, 1), padding (0, 1, 1)
  - Output channel # 64, kernel (3, 3, 3), stride (2, 1, 1), padding (1, 1, 1)

- Output:$C^{'} \times D^{'} \times H^{'} \times W^{'} $
  - Answer is $64\times2\times400\times352$

- Reshape into 2D feature map $2C^{'} \times  H^{'} \times W^{'}$
  - This is image-like **feature map**



### Region Proposal Network (RPN)

- Two anchor boxes
  - $x,y,z,l,w,h,\theta=0^o$
  - $x,y,z,l,w,h,\theta=90^o$

![rpn](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/rpn.1kx1vedkgf1c.png)

### Loss Function

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/cls.2oz22bl9rn40.jpg" alt="cls" style="zoom:50%;" /> <img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/reg.5g2zypi3ths0.jpg" alt="reg" style="zoom:50%;" />

$L_{cls} $ 是“二元交叉熵损失”——实际上不是一个标准的损失

- VoxelNet 仅用 1 个数字而不是 2 个数字来表示 “objectness confidence”

- $L_{cls}(p_i^{pos}) = log(\sigma(e^{p_i^{pos}}))$
- $L_{cls}(p_i^{neg}) = log(1-\sigma(e^{p_i^{neg}}))$
- $Sigmoid \ \sigma(x)=\frac{e^x}{e^x+1} \in(0,1)$







- 如何定义 positive / negative anchor box

**eg.**

- **Positive anchor box**
  - $IoU > 0.5$ with **any** ground truth box 
  - Has highest IoU with a ground truth box (each ground truth box will be associated with at least one anchor box).
- **Negative anchor box**
  - $IoU < 0.35$ with **every** ground truth box 
- **Don’t care**
  - $0.35\leq IoU \leq 0.5$with **every** ground truth box



![loss](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/loss.5p168i5b6bg0.png)



### 数据增强

- 围绕向上轴的全局旋转
  - 通常是 $\theta \in [-\frac{\pi}4,\frac{\pi}4]$
  - 全局缩放通常 $s \in[0.95,1.05]$

- 全局的平移是没有必要的

- For each **ground truth box**:
  - 随机旋转它并均匀地指向它内部 $\theta \in [-\frac{\pi}{10},\frac{\pi}{10}]$
  - 随机平移 $[\Delta x,\Delta y,\Delta z], \Delta* \sim N(0,1)$
  - 如果增强后两个盒子发生碰撞，则放弃增强

![data](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/data.5k0sc6c07x40.png)



### KITTI

![kitti](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/kitti.5yhgqeyv48c0.png)



## PointPillars

![PointPillars](https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/PointPillars.5361vd4lk2g0.jpg)

### Loss Function

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/l.j7by314lob4.jpg" alt="l" style="zoom:60%;" />

- $L_{loc}$：回归损失，几乎与 VoxelNet 相同：$\Delta \theta = \sin(\theta^g-\theta^g)$

- $L_{dir}$：$L_{loc}$ 中的 sin 不能区分 $180^o$ 的反转
  - 在离散化方向上添加 softmax 分类损失
  - 例如，将 $\theta$ 分成 4 个 bins，[ 0, 90 ], [90, 180], [180, 270], [270, 360]

- $L_{cls}$：类似于 VoxelNet ，但使用 FocalLoss 而不是 CrossEntropyLoss

## FocalLoss

**解决类别不均衡的问题**

例如：

- 1 ground truth “human” box
- Network predicts 100 boxes
  - $P_{background} = 0.6$ for the first 99 boxes
  - $P_{human} = 0.2$ for the $100^{th}$ boxes

- **Cross Entropy Loss**
  - $L_{CE} = \sum_t - logp_t = -99log0.6 -log0.2 = 50.57 + 1.61$
- 错误框分类（第100个框）造成的损失为1.61
- 正确框分类（99 个框）造成的损失为 50.57
- 简单，网络只是将一切预测为背景！



- **Focal Loss**
  - $L_{EL} = \sum_t - (1-p_t)^{\gamma}logp_t = -99(1-0.6)^2log0.6 -(1-0.2)^2log0.2 = 8.09 + 1.03$

- 错误分类造成的损失现在占据的比例更大。

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/fl.5d96qn8gvnc0.jpg" alt="fl" style="zoom:43%;" />

- Focal Loss 是为不平衡分类而设计的
- 减少分类良好的示例的相对损失（$p_t > 0.5$）
- 专注于困难的、错误分类的例子（$p_t \leq 0.5$）

