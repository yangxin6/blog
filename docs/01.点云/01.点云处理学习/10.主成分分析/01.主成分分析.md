---
title: 主成分分析
date: 2021-09-01 13:00:00
permalink: /pages/2UnIMe
categories: 
  - 点云
tags: 
  - 特征分解
author: 
  name: yangxin
  link: https://github.com/yangxin6/3DPointCloud
---
# 基础知识

## 特征值和特征向量

$A x=\lambda x$

其中$A$是一个 $n\times n$  的实对称矩阵，$x$ 是一个 $n$ 维向量，则我们说是 $\lambda$ 矩阵$A$的一个特征值，而 $x$ 是矩阵 $A$ 的特征值 $\lambda$ 所对应的<mark>特征向量</mark>。

矩阵 $A$ 的 $n$ 个特征值$\lambda _1≤\lambda _2≤...≤\lambda _n$, 以及这 $n$ 个特征值所对应的特征向量$\{\omega_1,\omega_2,...\omega_n\}$，如果这$n$个特征向量线性无关，那么矩阵$A$就可以用下式的特征分解表示:

$A=W \Sigma W^{−1}$

其中$W$是这$n$个特征向量所张成的$n×n$维矩阵，而 $\Sigma$ 为这$n$个特征值为主对角线的$n×n$维矩阵。

一般我们会把$W$的这$n$个特征向量标准化，即满足$||\omega_i||_2=1, ||w_i||_2=1$, 或者说$\omega^T_i\omega_i=1$，此时$W$的$n$个特征向量为标准<mark>正交基</mark>，满足$W^TW=I$，即$W^T=W^{−1}$, 也就是说$W$为<mark>酉矩阵</mark>。这样我们的特征分解表达式可以写成:

$A=W\Sigma W^T$

## 奇异值分解 (SVD)

SVD（ Singular Value Decomposition）也是对矩阵进行分解，SVD不要求要分解的矩阵为方阵。假设我们的矩阵A是一个$m×n$的矩阵，那么我们定义矩阵A的SVD为：

$A=U\Sigma V^T$

其中U是一个$m×m$的矩阵， $\Sigma$ 是一个$m×n$的矩阵，除了主对角线上的元素以外全为0，<mark>主对角线</mark>上的每个元素都称为<mark>奇异值</mark>，$V$是一个$n×n$的矩阵。$U$和$V$都是<mark>酉矩阵</mark>，即满足 $U^TU=I,V^TV=I,U^TU=I,V^TV=I$。下图可以很形象的看出上面SVD的定义：

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/1042406-20170105115457425-1545975626.76l3jj3sr6o0.png" style="zoom:75%;" />

### 求SVD分解后的$U,\Sigma ,V$这三个矩阵

如果我们将$A$的转置和$A$做矩阵乘法，那么会得到$n×n$的一个方阵$A^TA$。既然$ATA$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：

$(A^TA)v_i=\lambda _iv_i$

这样我们就可以得到矩阵$A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将$A^TA$的所有特征向量张成一个$n×n$的矩阵$V$，就是我们SVD公式里面的$V$矩阵了。一般我们将$V$中的每个特征向量叫做<mark>$A$ 的右奇异向量</mark>。

如果我们将$A$和$A$的转置做矩阵乘法，那么会得到$m×m$的一个方阵$AA^T$。既然$AA^T$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：

$(AA^T)u_i=\lambda _iu_i$

这样我们就可以得到矩阵$AA^T$的m个特征值和对应的m个特征向量$u$了。将$AA^T$的所有特征向量张成一个$m×m$的矩阵$U$，就是我们SVD公式里面的$U$矩阵了。一般我们将$U$中的每个特征向量叫做<mark>$A$的左奇异向量</mark>。

$U$和$V$我们都求出来了，现在就剩下奇异值矩阵$\Sigma$没有求出了。由于$\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\Sigma$就可以了。

我们注意到:

$A=U\Sigma V^T\Rightarrow AV=U\Sigma V^TV\Rightarrow AV=U\Sigma \Rightarrow Av_i=\Sigma _iu_i\Rightarrow \Sigma _i=Av_i/u_i$

 这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵$\Sigma$。

上面还有一个问题没有讲，就是我们说$A^TA$的特征向量组成的就是我们SVD中的$V$矩阵，而$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵，这有什么根据吗？这个其实很容易证明，我们以$V$矩阵的证明为例。

$A=U\Sigma V^T\Rightarrow A^T=V\Sigma ^TU^T\Rightarrow A^TA=V\Sigma ^TU^TU\Sigma V^T=V\Sigma ^2V^T$

上式证明使用了: $U^TU=I,\Sigma ^T\Sigma =\Sigma ^2$。可以看出$A^TA$的特征向量组成的的确就是我们SVD中的$V$矩阵。类似的方法可以得到$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵。

进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：

$\Sigma _i=\sqrt{\lambda _i}$

这样也就是说，我们可以不用$\Sigma _i=Av_i/u_i$来计算奇异值，也可以通过<mark>求出$ATA$的特征值取平方根来求奇异值</mark>。

### SVD的一些性质

对于奇异值，它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用<mark>最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵</mark>。也就是说：

$A_{m×n}=U_{m×m}\Sigma _{m×n}V_{n×n}^T≈U_{m×k}\Sigma _{k×k}V_{k×n}^T$

其中$k$要比$n$小很多，也就是一个大的矩阵$A$可以用三个小的矩阵$U_{m×k}\Sigma _{k×k}V_{k×n}^T$来表示。如下图所示，现在我们的矩阵$A$只需要灰色的部分的三个小矩阵就可以近似描述了。

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/1042406-20170105140822191-1774139119.52f9pl3lqac0.png" style="zoom:75%;" />



```md
::: 致谢
特征值和特征向量以及奇异值分解，借鉴于 [刘建平Pinard](https://www.cnblogs.com/pinard/p/6251584.html) 作者的博客
:::
```



## 谱定理

设对称矩阵 $A \in R^{n,n}$, $\lambda_i \in R, i=1,2,...,n$，$n$是 $A$ 的特征值，存在一个正交矩阵 $U$ ($UU^T=U^TU=I$)

$A=U\Lambda U^T = \sum_{i=1}^n \lambda _iu_iu_i^T ,\Lambda = diag(\lambda_1,...,\lambda_n)$

::: tip
注：diag() 表示对角阵
:::

## 瑞丽商

设一个对称矩阵 $A\in S^n$

- $\lambda_{min}(A) \le \frac{x^TAx}{x^Tx} \le \lambda_{max}(A), \forall x \neq 0$

- $\lambda_{max}(A)=\max \limits_{x:||x||_2=1} x^TAx$

- $\lambda_{min}(A)=\min \limits_{x:||x||_2=1} x^TAx$

# 主成分分析

**PCA(Principle Component Analysis) 主成份分析**

线性、一维

## 过程

输入： $x_i \in R^n, i= 1,2,...,m$

输出：主要向量 $z_i,z_2,...,z_k, k\le n$

步骤：

1. 归一化： $\tilde{X} = [\tilde{x_1},...,\tilde{x_m}], \tilde{x_i}=x_i-\bar{x},i=1,...,m$   , $\bar{x} = \frac{1}{m} \sum_{i=1}^m x_i$

2. SVD：$H = \tilde{X} \tilde{X}^T = U_\gamma\Sigma^2U_\gamma^T$
3. 得到主成份向量为 $U_\gamma$ 的列向量

## 应用

- 降维、压缩
- 分类

# 核主成分分析

**Kernel PCA**

非线性、高维

## 过程

- 输入：$x_i \in R^{n_0}$，存在一个映射 $\phi: R^{n_0} \rightarrow R^{n_1}$  （关键便是找 映射 $\phi$）

<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/maping.2ywofg4npms0.jpeg" alt="非线性映射" style="zoom:40%;" />

映射后的数据如图右边所示。可以看到，映射到高维空间后数据变得线性可分。这意味着，非线性映射 $\phi$ 使得我们能够处理非线性的数据。

- 在高维空间上遵循 线性 PCA
  1. 假设 $\phi(x_i)$ 已经 zero-center （零中心归一化）   $\frac{1}{N} \sum\limits^N_{i=1} \phi(x_i)=0$
  2. 计算关系矩阵 $\tilde{H} = \frac{1}{N} \sum\limits^N_{i=1} \phi(x_i) \phi(x_i)^T$
  3. 求特征值、特征向量 $\tilde{H}\tilde{z} = \tilde{\lambda}\tilde{z}$



## 核的选择

1. Linear              $k(x_i,x_j)=x_i^Tx_j$                  线性
2. Polynomial     $k(x_i,x_j)=(1+x_i^Tx_j)^p$     多项式
3. Gaussian         $k(x_i,x_j)=e^{-\beta||x_i-x_j||_2}$       高斯
4. Laplacian        $k(x_i,x_j)=e^{-\beta||x_i-x_j||_1}$       拉普拉斯

## 总结

1. 选一个 kernel $k(x_i,x_j)$ ， 组合为 Gram 矩阵 $K(i,j)=k(x_i,x_j)$
2. 归一化 $K$  （为了是高维数据点的平均值也为0）<img src="https://cdn.jsdelivr.net/gh/yangxin6/img-hosting@master/images/K.5k3chzer2us0.png" alt="K" style="zoom:25%;" />
3. 求 $\tilde{K}$ 的特征值和特征向量   $\tilde{K} \alpha_r= \lambda_r \alpha_r$
4. 归一化 $\alpha_r$ 使   $\alpha_r^T \alpha_r = \frac{1}{\lambda_r}$
5.  $\forall x \in R^n$ ,  $x$ 在$r$上的主向量为  $y_r = \phi^T(x)\tilde{z_r} = \sum\limits^N_{j=1} \alpha_{rj}k(x_i,x_j), y \in R$



## 思想

kernel PCA 核心思想 **<mark>把高维空间的运算转化为地位空间的核函数</mark>**（kernel）

## 应用

- 找点云中每个点的法向量
- 去噪
- 带权重的法向量估计
